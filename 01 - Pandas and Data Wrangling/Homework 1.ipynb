{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1\"><a href=\"#Task-1.-Compiling-Ebola-Data\"><span class=\"toc-item-num\">Task 1.&nbsp;&nbsp;</span>Compiling Ebola Data</a></div>\n",
    " <div class=\"lev1\"><a href=\"#Task-2.-RNA-Sequences\"><span class=\"toc-item-num\">Task 2.&nbsp;&nbsp;</span>RNA Sequences</a></div>\n",
    " <div class=\"lev1\"><a href=\"#Task-3.-Class-War-in-Titanic\"><span class=\"toc-item-num\">Task 3.&nbsp;&nbsp;</span>Class War in Titanic</a></div></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = '/home/robin/GIT/ADA2017-Tutorials/02 - Intro to Pandas/Data' # Use the data folder provided in Tutorial 02 - Intro to Pandas.\n",
    "print(DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Compiling Ebola Data\n",
    "\n",
    "The `DATA_FOLDER/ebola` folder contains summarized reports of Ebola cases from three countries (Guinea, Liberia and Sierra Leone) during the recent outbreak of the disease in West Africa. For each country, there are daily reports that contain various information about the outbreak in several cities in each country.\n",
    "\n",
    "Use pandas to import these data files into a single `Dataframe`.\n",
    "Using this `DataFrame`, calculate for *each country*, the *daily average per month* of *new cases* and *deaths*.\n",
    "Make sure you handle all the different expressions for *new cases* and *deaths* that are used in the reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "import glob\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here\n",
    "\n",
    "# Method to concat all the reports files for each country in a single dataframe\n",
    "\n",
    "def concat_files(path,date_str):\n",
    "    the_files = glob.glob(path +\"/*.csv\")\n",
    "    data_frame = pd.DataFrame()\n",
    "    for x in the_files:\n",
    "        df_temp = pd.read_csv(x,parse_dates=[date_str])\n",
    "        data_frame = pd.concat([data_frame,df_temp])\n",
    "    return data_frame\n",
    "\n",
    "# Todo: remove method select_rows for better readability\n",
    "# Method to select all the rows matching the 'wanted' argument\n",
    "def select_rows(df,reference,wanted):\n",
    "    return df.loc[df[reference] == wanted]\n",
    "\n",
    "# Method to add a Month column\n",
    "def add_month(df):\n",
    "    copy_df = df.copy()\n",
    "    months = [calendar.month_name[x.month] for x in copy_df.Date]\n",
    "    copy_df['Month'] = months\n",
    "    return copy_df\n",
    "\n",
    "# Method to calculate the average number of new cases per month\n",
    "def calculate_avg_new_cases(df,months):\n",
    "    avg_cases = np.zeros(len(months))\n",
    "    for i in range(len(months)):\n",
    "        temp = select_rows(df,'Month',months[i]).Total_new_cases.values.astype(float) \n",
    "        if(len(temp) < 2 ): # if less than two points, assign NaN\n",
    "            avg_cases[i] = np.nan\n",
    "        else: # Otherwise sum/number\n",
    "            avg_cases[i] = temp.sum()/temp.shape[0]\n",
    "        if avg_cases[i] < 0:\n",
    "            raise ValueError('Value of an average is less than 0')\n",
    "    return avg_cases  \n",
    "\n",
    "# Method to calculate the average number of deaths per month\n",
    "def calculate_avg_new_deaths(df,months):\n",
    "    avg_cases = np.zeros(len(months))\n",
    "    for i in range(len(months)):\n",
    "        temp = select_rows(df,'Month',months[i]) # selecting relevant month\n",
    "        temp = temp.reset_index()\n",
    "        delta_t = temp['Date'].loc[len(temp)-1] - temp['Date'].loc[0] # calculate time-span of data\n",
    "        temp = temp.Total_new_deaths.values.astype(float) # extracting array of floats\n",
    "        if(temp.shape[0]<2):\n",
    "            avg_cases[i] = np.nan\n",
    "        elif(temp[len(temp)-1]-temp[0] > 0):   # if there is a difference between 1st/last time point\n",
    "            avg_cases[i] = (temp[len(temp)-1]-temp[0])/delta_t.days\n",
    "        else: \n",
    "            avg_cases[i] = (temp[len(temp)-2]-temp[0])/delta_t.days\n",
    "        if avg_cases[i] < 0:\n",
    "            raise ValueError('Value of an average is less than 0')\n",
    "    return avg_cases   \n",
    "\n",
    "# Method to handle Sierra Leon and Liberia data\n",
    "def handle_data(df,desc1,desc2,desc3,new_total):\n",
    "    sl_1 = select_rows(df,'Description',desc1)\n",
    "    sl_2 = select_rows(df,'Description',desc2)\n",
    "    sl_3 = select_rows(df,'Description',desc3)\n",
    "    new_cases_registered =  [int(a) + int(b) + int(c) for a,b,c in zip(sl_1.Totals,sl_2.Totals,sl_3.Totals)]\n",
    "    sl_1[new_total] = new_cases_registered\n",
    "    sl_1 = sl_1[['Date',new_total]]\n",
    "    #sl_1 = add_month(sl_1,'Date')\n",
    "    return sl_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Guinea DF\n",
    "guinea_folder = DATA_FOLDER + '/ebola/guinea_data/'\n",
    "g_df = concat_files(guinea_folder,'Date')\n",
    "g_df.sort_values('Date', inplace=True)\n",
    "print('Shape of Guinea DF:', g_df.shape)\n",
    "\n",
    "# We can check all unique descriptions in order to choose what \n",
    "# to base our calculations on as well as to be sure not to miss anything\n",
    "g_df['Description'].unique()\n",
    "\n",
    "# We can then see which descriptors appear to be the most complete,\n",
    "# in this case it seems using one of the total deaths descriptors\n",
    "# is the only sensible option, the new deaths data sets are much\n",
    "# less complete. We'll take the confirmed+probables+suspects\n",
    "# in order to be most complete, so the end results are probably\n",
    "# overestimating the deaths a bit\n",
    "print(g_df.loc[g_df['Description']=='Total deaths of probables'].shape)\n",
    "print(g_df.loc[g_df['Description']=='Total deaths of confirmed'].shape)\n",
    "print(g_df.loc[g_df['Description']=='Total deaths (confirmed + probables + suspects)'].shape)\n",
    "print(g_df.loc[g_df['Description']=='New deaths registered'].shape)\n",
    "print(g_df.loc[g_df['Description']=='New deaths registered today'].shape)\n",
    "print(g_df.loc[g_df['Description']=='New deaths registered today (probables)'].shape)\n",
    "print(g_df.loc[g_df['Description']=='New deaths registered today (suspects)'].shape)\n",
    "g_df.loc[g_df['Description']=='Total deaths (confirmed + probables + suspects)'][['Date','Description','Totals']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only keep the columns of interest\n",
    "g_df = g_df[['Date', 'Description', 'Totals']]\n",
    "\n",
    "# We select the rows that we need in order to calculate our values\n",
    "g_new_cases = select_rows(g_df,'Description','Total new cases registered so far')\n",
    "g_new_deaths = select_rows(g_df,'Description','Total deaths (confirmed + probables + suspects)')\n",
    "g_new_deaths.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling the new Dataframes\n",
    "g_new_cases = g_new_cases[['Date','Totals']]\n",
    "g_new_cases.columns = ['Date','Total_new_cases']\n",
    "\n",
    "g_new_deaths = g_new_deaths[['Date','Totals']]\n",
    "g_new_deaths.columns = ['Date','Total_new_deaths']\n",
    "\n",
    "# We merge the 2 of them to obtain the new cases and the deaths for Guinea\n",
    "guinea_df = pd.merge(g_new_cases,g_new_deaths)\n",
    "guinea_df['Country'] = 'Guinea'\n",
    "guinea_df = guinea_df[['Country','Date','Total_new_cases','Total_new_deaths']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_months = add_month(guinea_df)\n",
    "g_months.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = select_rows(g_months,'Month','September')\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally get to calculate average daily new cases and deaths per month.\n",
    "In this data, we assume that the *Total deaths (confirmed + probables + suspects)* row we used from the original data is a cumulative value of the deaths.\n",
    "However, the case for the *Total new cases registered so far* column was not as clear. The values for August appear to increasee monotonically as a a cumulative variable would. While the values for September seem to fluctuate more. We therefore chose to interpret this column as a *daily* new cases registered value, as this is compatible with the fluctuations observed in September."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the daily average per month of new cases and deaths for Guinea\n",
    "months = ['August','September','October']\n",
    "g_results_deaths = calculate_avg_new_deaths(g_months,months)\n",
    "print(g_results_deaths)\n",
    "g_results_cases = calculate_avg_new_cases(g_months,months)\n",
    "g_results_cases\n",
    "\n",
    "# NaN values is for the months were we dont have enough data to make an average,\n",
    "# in this case there is only one data point for October, so we chose to exclude it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Sierra Leone DataFrame\n",
    "sl_folder = DATA_FOLDER + '/ebola/sl_data/'\n",
    "sl_df = concat_files(sl_folder,'date')\n",
    "sl_df.sort_values('date', inplace=True)\n",
    "sl_df[['date' , 'variable' , 'National']]\n",
    "\n",
    "# We only keep the columns of interest \n",
    "sl_df = sl_df[['date','variable','National']]\n",
    "sl_df.columns = ['Date' , 'Description' , 'Totals']\n",
    "sl_df = sl_df.fillna(0)\n",
    "\n",
    "# Creating dataframes for new cases and new deaths\n",
    "sl_new_cases = handle_data(sl_df,'new_confirmed','new_probable','new_suspected','Total_new_cases')\n",
    "sl_new_cases1 = sl_new_cases.loc[sl_new_cases['Total_new_cases'] > 0]\n",
    "sl_new_deaths = handle_data(sl_df,'death_confirmed','death_probable','death_suspected','Total_new_deaths')\n",
    "sl_new_deaths1 = sl_new_deaths.loc[sl_new_deaths['Total_new_deaths'] > 0]\n",
    "\n",
    "# Forming the Sierra Leon DataFrame\n",
    "sierra_leon_df = pd.merge(sl_new_cases,sl_new_deaths)\n",
    "sierra_leon_df['Country'] = 'Sierra Leone'\n",
    "sl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the daily average per month of new cases and deaths for Sierra Leon\n",
    "months = ['August','September','October','November','December']\n",
    "sl_results_deaths = calculate_avg_new_deaths(add_month(sl_new_deaths1,),months)\n",
    "sl_results_deaths\n",
    "sl_results_cases = calculate_avg_new_cases(add_month(sl_new_cases1,),months)\n",
    "sl_results_cases\n",
    "sl_results_deaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Liberia DataFrame\n",
    "lib_folder = DATA_FOLDER + '/ebola/liberia_data/'\n",
    "lib_df = concat_files(lib_folder,'Date')\n",
    "lib_df.sort_values('Date', inplace=True)\n",
    "\n",
    "# We only keep the columns of interest \n",
    "lib_df = lib_df[['Date','Variable','National']]\n",
    "lib_df.columns = ['Date' , 'Description' , 'Totals']\n",
    "lib_df = lib_df.fillna(0)\n",
    "lib_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframes for new cases and new deaths\n",
    "lib_new_cases = handle_data(lib_df,'New Case/s (Suspected)','New Case/s (Probable)','New Case/s (Probable)','Total_new_cases')\n",
    "lib_new_cases1 = lib_new_cases.loc[lib_new_cases['Total_new_cases'] > 0]\n",
    "lib_new_deaths = handle_data(lib_df,'Total death/s in confirmed cases','Total death/s in probable cases','Total death/s in suspected cases','Total_new_deaths')\n",
    "lib_new_deaths1 = lib_new_deaths.loc[lib_new_deaths['Total_new_deaths'] > 0]\n",
    "lib_new_cases1\n",
    "lib_new_deaths1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forming the Sierra Leon DataFrame\n",
    "liberia_df = pd.merge(lib_new_cases,lib_new_deaths)\n",
    "liberia_df['Country'] = 'Liberia'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the daily average per month of new cases and deaths for Liberia\n",
    "months = ['June','July','August','September','October','November','December']\n",
    "lib_results_deaths = calculate_avg_new_deaths(add_month(lib_new_deaths1,),months)\n",
    "lib_results_deaths\n",
    "lib_results_cases = calculate_avg_new_cases(add_month(lib_new_cases1,),months)\n",
    "lib_results_cases\n",
    "#lib_results_deaths\n",
    "#liberia_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forming the Final DataFrame contaning the relevant information for the 3 different country\n",
    "\n",
    "df = pd.concat([guinea_df,sierra_leon_df,liberia_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. RNA Sequences\n",
    "\n",
    "In the `DATA_FOLDER/microbiome` subdirectory, there are 9 spreadsheets of microbiome data that was acquired from high-throughput RNA sequencing procedures, along with a 10<sup>th</sup> file that describes the content of each. \n",
    "\n",
    "Use pandas to import the first 9 spreadsheets into a single `DataFrame`.\n",
    "Then, add the metadata information from the 10<sup>th</sup> spreadsheet as columns in the combined `DataFrame`.\n",
    "Make sure that the final `DataFrame` has a unique index and all the `NaN` values have been replaced by the tag `unknown`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we are going to concatenate all excel files into one `DataFrame`. We'll make sure we have a unique index by resetting it to a temporary 0-x numerical index. We're going to be using the variable `xl_lengths` to keep track of the size of the different tables that are concatenated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_data = pd.DataFrame(columns=['raw','counts'])\n",
    "nb_excels = 9\n",
    "xl_lengths = np.zeros(nb_excels, dtype=int)\n",
    "counts = 0\n",
    "\n",
    "# Concat all excel files\n",
    "for i in range(nb_excels):\n",
    "    mb_new = pd.read_excel(DATA_FOLDER+\"/microbiome/MID\"+str(i+1)+\".xls\", header=None, names=['raw','counts'])\n",
    "    mb_new.fillna('unknown', inplace=True)\n",
    "    xl_lengths[i] = np.size(mb_new.index)\n",
    "    counts += mb_new['raw'].size\n",
    "    mb_data = pd.concat([mb_data, mb_new], axis=0, join='outer')\n",
    "\n",
    "# Reset indices\n",
    "mb_data = mb_data.reset_index() # creates a new 'index' column\n",
    "mb_data = mb_data.drop('index', axis=1) # dropping this new column\n",
    "\n",
    "print('Nb. of data points:', counts)\n",
    "print('Data frame shape', mb_data.shape)\n",
    "mb_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read in the metadata :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_metadata = pd.read_excel(DATA_FOLDER+\"/microbiome/metadata.xls\")\n",
    "mb_metadata.fillna('unknown', inplace=True)\n",
    "print('Metadata DF index is unique:',mb_metadata.index.is_unique)\n",
    "mb_metadata.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now initialise the new columns that will be filled in with the metadata information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_data = pd.concat([mb_data, pd.DataFrame(columns=mb_metadata.columns)], axis=1)\n",
    "print('Data frame shape:',mb_data.shape)\n",
    "mb_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now add the metadata to the corresponding slices of the overall `DataFrame` using our previous `xl_length` variable to keep track of when the imported tables end and another begins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "for j in range(np.size(xl_lengths)):\n",
    "    if j >= 1:\n",
    "        idx = sum(xl_lengths[0:j])\n",
    "    for i in mb_metadata.columns:\n",
    "        mb_data[i][idx:idx+xl_lengths[j]+1] = mb_metadata[i][j]\n",
    "#print(mb_data[mb_metadata.columns])#[0:xl_lengths[0]+10])\n",
    "\n",
    "print(xl_lengths)\n",
    "print('Final DataFrame index is unique', mb_data.index.is_unique)\n",
    "mb_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a meaningful index using all the metadata that we imported and joined together. We'll also check that the index is unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_index = ['raw', 'BARCODE', 'GROUP', 'SAMPLE']\n",
    "mb_data = mb_data.set_index(new_index)\n",
    "print('Index is unique: ', mb_data.index.is_unique)\n",
    "mb_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now look at all the data concerning one type of organism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_data.loc['Archaea \"Crenarchaeota\" Thermoprotei Desulfurococcales Desulfurococcaceae Ignisphaera']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on how we wish to view the combined data frame, for example all tissue samples of a specific organism, we can swap indices to make this easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_data = mb_data.swaplevel('BARCODE','SAMPLE')\n",
    "mb_data.sort_index(axis=0, inplace=True) # without sorting a PerformanceWarning is thrown\n",
    "mb_data.loc['Archaea \"Crenarchaeota\" Thermoprotei Desulfurococcales Desulfurococcaceae Ignisphaera', 'tissue']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Class War in Titanic\n",
    "\n",
    "Use pandas to import the data file `Data/titanic.xls`. It contains data on all the passengers that travelled on the Titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(filename=DATA_FOLDER+'/titanic.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(DATA_FOLDER+'/titanic.xls', sheetname='titanic', header=0)\n",
    "print('Shape of DataFrame:', df.shape)\n",
    "print(type(df['sex'][0]))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For each of the following questions state clearly your assumptions and discuss your findings:\n",
    "1. Describe the *type* and the *value range* of each attribute. Indicate and transform the attributes that can be `Categorical`. \n",
    "2. Plot histograms for the *travel class*, *embarkation port*, *sex* and *age* attributes. For the latter one, use *discrete decade intervals*. \n",
    "3. Calculate the proportion of passengers by *cabin floor*. Present your results in a *pie chart*.\n",
    "4. For each *travel class*, calculate the proportion of the passengers that survived. Present your results in *pie charts*.\n",
    "5. Calculate the proportion of the passengers that survived by *travel class* and *sex*. Present your results in *a single histogram*.\n",
    "6. Create 2 equally populated *age categories* and calculate survival proportions by *age category*, *travel class* and *sex*. Present your results in a `DataFrame` with unique index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question: Describe the type and the value range of each attribute. \n",
    "\n",
    "# We assume that we can describe the value range of each attribute\n",
    "# only if it's a numerical type\n",
    "\n",
    "types = df.dtypes\n",
    "value_range = df._get_numeric_data().apply(lambda x: (x.min(), x.max()))\n",
    "\n",
    "types.index.name = 'Attribute'\n",
    "types.columns = 'Type'\n",
    "\n",
    "value_range.index.name = 'Attribute'\n",
    "value_range.columns = 'Range'\n",
    "\n",
    "res = pd.concat(dict(Range=value_range, Type=types), axis=1).fillna('-')\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question: Indicate and transform the attributes that can be Categorical.\n",
    "\n",
    "# By visual inspection we can argue that the following columns could be\n",
    "# categorical : 'pclass', 'survived', 'sex', 'sibsp', 'parch', 'embarked'\n",
    "#\n",
    "# We assumed that for an attribute to be categorical it should have \n",
    "# <= 10 distinct elements. (NaN is not considered as a distinct value)\n",
    "#\n",
    "# In this case, both these criteria lead to the same selection of\n",
    "# attributes\n",
    "#\n",
    "# We chose to leave out 'cabin' as a category due to the large number\n",
    "# of values that are present within it, especially compared to other\n",
    "# columns such as those mentioned above \n",
    "\n",
    "\n",
    "#categories = [v for v in df.columns if (pd.Series(['unique']).isin(df[v].describe().index.tolist())).bool()\n",
    "                                            #and df[v].describe()['unique'] < 10 ]\n",
    "categories = [v for v in df.columns if np.size(df[v].unique()) < 10 ]\n",
    "\n",
    "print('Attributes to be made Categorical:', categories)\n",
    "for i in categories:\n",
    "    df[i] = df[i].astype('category')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question: Plot a histogram for the travel class attribute\n",
    "\n",
    "#xs = [1, 2, 3]\n",
    "#l = df['pclass'].tolist()\n",
    "#ys = [l.count(1), l.count(2), l.count(3)]\n",
    "\n",
    "temp = []\n",
    "[temp.append(v) for v in df['pclass'].index if not df['pclass'][v] in temp]\n",
    "\n",
    "\n",
    "hist_plot = df.pclass.value_counts().plot(kind='bar')\n",
    "hist_plot.set_title('Travel Class')\n",
    "hist_plot.set_xlabel('Class')\n",
    "hist_plot.set_ylabel('Passengers')\n",
    "np.sum(df.pclass.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question: Plot a histogram for the embarkation port attribute\n",
    "\n",
    "nb_ports = df['embarked'].describe()['unique']\n",
    "df.embarked.unique()\n",
    "\n",
    "hist_plot = df.embarked.value_counts().plot(kind='bar')\n",
    "hist_plot.set_title('Embarkation port')\n",
    "hist_plot.set_xlabel('Port')\n",
    "hist_plot.set_ylabel('Passengers')\n",
    "sum(df.embarked.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question: Plot a histogram for the sex attribute\n",
    "\n",
    "hist_plot = df.sex.value_counts().plot(kind='bar')\n",
    "hist_plot.set_title('Sex')\n",
    "hist_plot.set_xlabel('Sex')\n",
    "hist_plot.set_ylabel('Passengers')\n",
    "np.sum(df.sex.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question: Plot a histogram for the age attribute\n",
    "\n",
    "nb_bins = np.ceil(df.age.max()/10).astype('int')\n",
    "hist_plot = df.age.hist(bins=nb_bins, grid=False, xlabelsize=11, ylabelsize=11, figsize=(10, 6))\n",
    "hist_plot.set_title('Age')\n",
    "hist_plot.set_xlabel('Ages')\n",
    "hist_plot.set_ylabel('Passengers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question: Calculate the proportion of passengers by cabin floor. \n",
    "# Present your results in a pie chart.\n",
    "\n",
    "# We assumed that the information on the cabin floor is available \n",
    "# in the column 'cabin' where the letter indicates the floor.\n",
    "# the number that follows indicates a specific cabin on that floor.\n",
    "\n",
    "# Unfortunately, this column is also filled with NaN's values.\n",
    "# For this statistic we won't try guessing the cabin floor for \n",
    "# passenger where the data is not provided. Our analysis relies\n",
    "# only on values that are already in the dataset.\n",
    "\n",
    "tot = df.cabin.describe()['count']\n",
    "\n",
    "temp = df.cabin.dropna(axis=0).apply(lambda x: str(x)[:1])\n",
    "\n",
    "D = {}\n",
    "for v in temp:\n",
    "    if not v in D:\n",
    "        D[v] = 1\n",
    "    else:\n",
    "        D[v] += 1\n",
    "\n",
    "\n",
    "labels = list(D.keys())\n",
    "values = list(D.values())\n",
    "\n",
    "\n",
    "plt.pie(values, labels=labels)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question: For each travel class, calculate the proportion of the\n",
    "# passengers that survived. Present your results in pie charts.\n",
    "\n",
    "tot = df.survived.describe()['count']\n",
    "\n",
    "class_tot = (df.groupby('pclass').survived.describe())['count']\n",
    "pclass_survived = df.groupby(['pclass', 'survived']).survived.describe()\n",
    "pclass_survived = (pclass_survived[pclass_survived['top'] == 1])['freq']\n",
    "pclass_survived = pclass_survived.reset_index('survived').drop('survived', axis=1)\n",
    "stats = pd.concat([class_tot, pclass_survived], axis=1)\n",
    "prop = stats['freq'] / stats['count']\n",
    "\n",
    "l = df.set_index('pclass').index.categories\n",
    "plt.pie(list(prop), labels=list(l))\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
